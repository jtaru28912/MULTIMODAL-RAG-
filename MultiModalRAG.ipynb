{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOi6vPONXtTOY2kIJhuwHqX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jtaru28912/MULTIMODAL-RAG-SYSTEM/blob/main/MultiModalRAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multi-modal RAG System\n",
        "\n",
        "Many documents contain a mixture of content types, including text, tables and images.\n",
        "\n",
        "Yet, information captured in images is lost in most RAG applications.\n",
        "\n",
        "With the emergence of multimodal LLMs, like [GPT-4o](https://openai.com/index/hello-gpt-4o/), it is worth considering how to utilize images in RAG Systems:"
      ],
      "metadata": {
        "id": "lqJqMyrPpEZ9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "![](https://i.imgur.com/wcCDT38.gif)\n"
      ],
      "metadata": {
        "id": "bShWnpauo89R"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ju04ybx2J-1o",
        "outputId": "c23bd039-1a6f-44f9-8f80-c992066136de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain==0.3.7 in /usr/local/lib/python3.12/dist-packages (0.3.7)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.12/dist-packages (from langchain==0.3.7) (6.0.3)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.12/dist-packages (from langchain==0.3.7) (2.0.46)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from langchain==0.3.7) (3.13.3)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.15 in /usr/local/lib/python3.12/dist-packages (from langchain==0.3.7) (0.3.83)\n",
            "Collecting langchain-text-splitters<0.4.0,>=0.3.0 (from langchain==0.3.7)\n",
            "  Using cached langchain_text_splitters-0.3.11-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting langsmith<0.2.0,>=0.1.17 (from langchain==0.3.7)\n",
            "  Using cached langsmith-0.1.147-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting numpy<2.0.0,>=1.26.0 (from langchain==0.3.7)\n",
            "  Using cached numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain==0.3.7) (2.12.3)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.12/dist-packages (from langchain==0.3.7) (2.32.5)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain==0.3.7) (9.1.4)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.7) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.7) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.7) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.7) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.7) (6.7.1)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.7) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.7) (1.22.0)\n",
            "INFO: pip is looking at multiple versions of langchain-core to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting langchain-core<0.4.0,>=0.3.15 (from langchain==0.3.7)\n",
            "  Using cached langchain_core-0.3.82-py3-none-any.whl.metadata (3.2 kB)\n",
            "  Using cached langchain_core-0.3.81-py3-none-any.whl.metadata (3.2 kB)\n",
            "  Using cached langchain_core-0.3.80-py3-none-any.whl.metadata (3.2 kB)\n",
            "  Using cached langchain_core-0.3.79-py3-none-any.whl.metadata (3.2 kB)\n",
            "  Using cached langchain_core-0.3.78-py3-none-any.whl.metadata (3.2 kB)\n",
            "  Using cached langchain_core-0.3.77-py3-none-any.whl.metadata (3.2 kB)\n",
            "  Using cached langchain_core-0.3.76-py3-none-any.whl.metadata (3.7 kB)\n",
            "INFO: pip is still looking at multiple versions of langchain-core to determine which version is compatible with other requirements. This could take a while.\n",
            "  Using cached langchain_core-0.3.75-py3-none-any.whl.metadata (5.7 kB)\n",
            "  Using cached langchain_core-0.3.74-py3-none-any.whl.metadata (5.8 kB)\n",
            "  Using cached langchain_core-0.3.73-py3-none-any.whl.metadata (5.8 kB)\n",
            "  Using cached langchain_core-0.3.72-py3-none-any.whl.metadata (5.8 kB)\n",
            "  Using cached langchain_core-0.3.71-py3-none-any.whl.metadata (5.8 kB)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "  Using cached langchain_core-0.3.70-py3-none-any.whl.metadata (5.8 kB)\n",
            "  Using cached langchain_core-0.3.69-py3-none-any.whl.metadata (5.8 kB)\n",
            "  Using cached langchain_core-0.3.68-py3-none-any.whl.metadata (5.8 kB)\n",
            "  Using cached langchain_core-0.3.67-py3-none-any.whl.metadata (5.8 kB)\n",
            "  Using cached langchain_core-0.3.66-py3-none-any.whl.metadata (5.8 kB)\n",
            "  Using cached langchain_core-0.3.65-py3-none-any.whl.metadata (5.8 kB)\n",
            "  Using cached langchain_core-0.3.64-py3-none-any.whl.metadata (5.8 kB)\n",
            "  Using cached langchain_core-0.3.63-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.12/dist-packages (from langchain-core<0.4.0,>=0.3.15->langchain==0.3.7) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.12/dist-packages (from langchain-core<0.4.0,>=0.3.15->langchain==0.3.7) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.12/dist-packages (from langchain-core<0.4.0,>=0.3.15->langchain==0.3.7) (4.15.0)\n",
            "Collecting langchain-text-splitters<0.4.0,>=0.3.0 (from langchain==0.3.7)\n",
            "  Using cached langchain_text_splitters-0.3.10-py3-none-any.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain==0.3.7) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain==0.3.7) (3.11.7)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain==0.3.7) (1.0.0)\n",
            "Collecting pip>=25.2 (from langchain-text-splitters<0.4.0,>=0.3.0->langchain==0.3.7)\n",
            "  Using cached pip-26.0.1-py3-none-any.whl.metadata (4.7 kB)\n",
            "Collecting langchain-text-splitters<0.4.0,>=0.3.0 (from langchain==0.3.7)\n",
            "  Using cached langchain_text_splitters-0.3.9-py3-none-any.whl.metadata (1.9 kB)\n",
            "  Using cached langchain_text_splitters-0.3.8-py3-none-any.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain==0.3.7) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain==0.3.7) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain==0.3.7) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain==0.3.7) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain==0.3.7) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain==0.3.7) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain==0.3.7) (2026.1.4)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3,>=1.4->langchain==0.3.7) (3.3.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.3.7) (4.12.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.3.7) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.3.7) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.15->langchain==0.3.7) (3.0.0)\n",
            "Using cached langchain_core-0.3.63-py3-none-any.whl (438 kB)\n",
            "Using cached langsmith-0.1.147-py3-none-any.whl (311 kB)\n",
            "Using cached langchain_text_splitters-0.3.8-py3-none-any.whl (32 kB)\n",
            "Using cached numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)\n",
            "Installing collected packages: numpy, langsmith, langchain-core, langchain-text-splitters\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: langsmith\n",
            "    Found existing installation: langsmith 0.7.5\n",
            "    Uninstalling langsmith-0.7.5:\n",
            "      Successfully uninstalled langsmith-0.7.5\n",
            "  Attempting uninstall: langchain-core\n",
            "    Found existing installation: langchain-core 0.3.83\n",
            "    Uninstalling langchain-core-0.3.83:\n",
            "      Successfully uninstalled langchain-core-0.3.83\n",
            "  Attempting uninstall: langchain-text-splitters\n",
            "    Found existing installation: langchain-text-splitters 1.1.1\n",
            "    Uninstalling langchain-text-splitters-1.1.1:\n",
            "      Successfully uninstalled langchain-text-splitters-1.1.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "chromadb 0.5.23 requires tokenizers<=0.20.3,>=0.13.2, but you have tokenizers 0.22.2 which is incompatible.\n",
            "langchain-classic 1.0.1 requires langchain-core<2.0.0,>=1.2.5, but you have langchain-core 0.3.63 which is incompatible.\n",
            "langchain-classic 1.0.1 requires langchain-text-splitters<2.0.0,>=1.1.0, but you have langchain-text-splitters 0.3.8 which is incompatible.\n",
            "langchain-community 0.4.1 requires langchain-core<2.0.0,>=1.0.1, but you have langchain-core 0.3.63 which is incompatible.\n",
            "pytensor 2.37.0 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "shap 0.50.0 requires numpy>=2, but you have numpy 1.26.4 which is incompatible.\n",
            "langgraph-prebuilt 1.0.7 requires langchain-core>=1.0.0, but you have langchain-core 0.3.63 which is incompatible.\n",
            "jaxlib 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "rasterio 1.5.0 requires numpy>=2, but you have numpy 1.26.4 which is incompatible.\n",
            "tobler 0.13.0 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-contrib-python 4.13.0.92 requires numpy>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python 4.13.0.92 requires numpy>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "jax 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python-headless 4.13.0.92 requires numpy>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed langchain-core-0.3.63 langchain-text-splitters-0.3.8 langsmith-0.1.147 numpy-1.26.4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "9f473779ace74051b69f75ee6ff1f81c"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install langchain==0.3.7"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-openai==0.2.8"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "203cxcM0L3Ca",
        "outputId": "7eea3632-541f-4381-e2ad-48775e482d05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain-openai==0.2.8 in /usr/local/lib/python3.12/dist-packages (0.2.8)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.17 in /usr/local/lib/python3.12/dist-packages (from langchain-openai==0.2.8) (0.3.63)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.54.0 in /usr/local/lib/python3.12/dist-packages (from langchain-openai==0.2.8) (1.109.1)\n",
            "Requirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.12/dist-packages (from langchain-openai==0.2.8) (0.12.0)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.126 in /usr/local/lib/python3.12/dist-packages (from langchain-core<0.4.0,>=0.3.17->langchain-openai==0.2.8) (0.1.147)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<0.4.0,>=0.3.17->langchain-openai==0.2.8) (9.1.4)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.12/dist-packages (from langchain-core<0.4.0,>=0.3.17->langchain-openai==0.2.8) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.12/dist-packages (from langchain-core<0.4.0,>=0.3.17->langchain-openai==0.2.8) (6.0.3)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.12/dist-packages (from langchain-core<0.4.0,>=0.3.17->langchain-openai==0.2.8) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.12/dist-packages (from langchain-core<0.4.0,>=0.3.17->langchain-openai==0.2.8) (4.15.0)\n",
            "Requirement already satisfied: pydantic>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain-core<0.4.0,>=0.3.17->langchain-openai==0.2.8) (2.12.3)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.54.0->langchain-openai==0.2.8) (4.12.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.54.0->langchain-openai==0.2.8) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.54.0->langchain-openai==0.2.8) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.54.0->langchain-openai==0.2.8) (0.13.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.54.0->langchain-openai==0.2.8) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.54.0->langchain-openai==0.2.8) (4.67.3)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken<1,>=0.7->langchain-openai==0.2.8) (2025.11.3)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.12/dist-packages (from tiktoken<1,>=0.7->langchain-openai==0.2.8) (2.32.5)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.54.0->langchain-openai==0.2.8) (3.11)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.54.0->langchain-openai==0.2.8) (2026.1.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.54.0->langchain-openai==0.2.8) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.54.0->langchain-openai==0.2.8) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.17->langchain-openai==0.2.8) (3.0.0)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith<0.4,>=0.1.126->langchain-core<0.4.0,>=0.3.17->langchain-openai==0.2.8) (3.11.7)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<0.4,>=0.1.126->langchain-core<0.4.0,>=0.3.17->langchain-openai==0.2.8) (1.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.4->langchain-core<0.4.0,>=0.3.17->langchain-openai==0.2.8) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.4->langchain-core<0.4.0,>=0.3.17->langchain-openai==0.2.8) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.4->langchain-core<0.4.0,>=0.3.17->langchain-openai==0.2.8) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai==0.2.8) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai==0.2.8) (2.5.0)\n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0m^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-community"
      ],
      "metadata": {
        "id": "x7Uy_BcHL3YA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-chroma==0.1.4"
      ],
      "metadata": {
        "id": "QnjxV744L3m3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install redis==5.2.0"
      ],
      "metadata": {
        "id": "aHWgb1GIL3tB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt') #punctuation mark\n",
        "nltk.download('punkt_tab') #punctuation tab\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "# The 'averaged_perceptron_tagger' is a Part-of-Speech (POS) tagger. It's used in Natural Language Processing (NLP)\n",
        "# to assign grammatical categories (like noun, verb, adjective) to words in a text.\n",
        "# This is crucial for tasks like text analysis, information extraction, and machine translation."
      ],
      "metadata": {
        "id": "cjTuf9PVL3z1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install \"unstructured[all-docs]\"\n",
        "# for parsing the data"
      ],
      "metadata": {
        "id": "OI_Mv9JYNJbv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# install OCR dependencies for unstructured image processing\n",
        "!sudo apt-get install tesseract-ocr\n",
        "!sudo apt-get install poppler-utils"
      ],
      "metadata": {
        "id": "fDp_5dA1NKDx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# extract table from pdf\n",
        "!pip install htmltabletomd==1.0.0"
      ],
      "metadata": {
        "id": "90zE45esNKRs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Loading & Processing\n",
        "\n",
        "### Partition PDF tables, text, and images\n",
        "  "
      ],
      "metadata": {
        "id": "FmgMJcJmRh1F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://sgp.fas.org/crs/misc/IF10244.pdf\n",
        "# website getting - download the pdf from online"
      ],
      "metadata": {
        "id": "s6g084frNKc4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This command removes the 'figures' directory and all its contents recursively and forcefully.\n",
        "# This is typically done to clean up previous outputs or temporary files.\n",
        "!rm -rf ./figures"
      ],
      "metadata": {
        "id": "KTm5lS5DNKnF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extractig images and tables from unstructured.io\n",
        "from langchain_community.document_loaders import UnstructuredPDFLoader\n",
        "\n",
        "doc = '/content/IF10244.pdf'\n",
        "# Extract tables\n",
        "# takes 1-2 min on Colab\n",
        "loader = UnstructuredPDFLoader(file_path=doc,\n",
        "                               strategy='hi_res',\n",
        "                               extract_images_in_pdf=True,\n",
        "                               infer_table_structure=True,\n",
        "                               mode='elements',\n",
        "                               image_output_dir_path='/content/figures')\n",
        "data = loader.load()"
      ],
      "metadata": {
        "id": "pw7q2XHHNKwT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(data)\n",
        "# data"
      ],
      "metadata": {
        "collapsed": true,
        "id": "jVq3tl3-NK6D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# extracting table here\n",
        "[doc.metadata['category'] for doc in data if doc.metadata['category'] == 'Table']"
      ],
      "metadata": {
        "id": "vhxeNAL8NLDw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tables = [doc for doc in data if doc.metadata['category'] == 'Table']\n",
        "len(tables)"
      ],
      "metadata": {
        "id": "iv7OiCq8NLM-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loader = UnstructuredPDFLoader(file_path=doc,\n",
        "                               strategy='hi_res',\n",
        "                               extract_images_in_pdf=True,\n",
        "                               infer_table_structure=True,\n",
        "                               chunking_strategy=\"by_title\", # section-based chunking\n",
        "                               max_characters=4000, # max size of chunks\n",
        "                               new_after_n_chars=4000, # preferred size of chunks\n",
        "                               overlap_n_chars=20, # overlap between chunks\n",
        "                               combine_text_under_n_chars=2000, # smaller chunks < 2000 chars will be combined into a larger chunk\n",
        "                               mode='elements',\n",
        "                               image_output_dir_path='./figures')\n",
        "texts = loader.load()\n",
        "len(texts)"
      ],
      "metadata": {
        "id": "Sm4Auym8NLXB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = texts + tables\n",
        "data\n",
        "len(data)\n",
        "data[5]"
      ],
      "metadata": {
        "collapsed": true,
        "id": "WrGBgC5jSMAw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import HTML, display, Markdown"
      ],
      "metadata": {
        "id": "ofVxEfNFSMN-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data[2].page_content)"
      ],
      "metadata": {
        "id": "SROrnAQyYmh7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data[1].metadata['text_as_html'])"
      ],
      "metadata": {
        "id": "kZkiJLVHYmwH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(Markdown(data[1].metadata['text_as_html']))"
      ],
      "metadata": {
        "id": "cuxFhuYOYm7a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Since unstructured extracts the text from the table without any borders, we can use the HTML text and put it directly in prompts (LLMs understand HTML tables well) or even better convert HTML tables to Markdown tables as below"
      ],
      "metadata": {
        "id": "mkR39FooYnGl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import htmltabletomd\n",
        "\n",
        "md_table = htmltabletomd.convert_table(data[1].metadata['text_as_html'])\n",
        "print(md_table)"
      ],
      "metadata": {
        "id": "B-pwZaXNYnQ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Separate Data into Text and Table Elements"
      ],
      "metadata": {
        "id": "z1R768RrZi8r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "docs = []\n",
        "tables = []\n",
        "\n",
        "for doc in data:\n",
        "  if doc.metadata['category'] =='Table':\n",
        "    tables.append(doc)\n",
        "  elif doc.metadata['category'] =='NarrativeText':\n",
        "    docs.append(doc)\n",
        "  elif doc.metadata['category'] =='Title':\n",
        "    docs.append(doc)\n",
        "  elif doc.metadata['category'] =='FigureCaption':\n",
        "    docs.append(doc)\n",
        "  elif doc.metadata['category'] =='UncategorizedText':\n",
        "    docs.append(doc)\n",
        "  elif doc.metadata['category'] =='Header':\n",
        "    docs.append(doc)\n",
        "  else:\n",
        "    docs.append(doc)"
      ],
      "metadata": {
        "id": "IiSTmB6nYnbk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(docs))\n",
        "print(len(tables))"
      ],
      "metadata": {
        "id": "r7_B-RbVYnlg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CONVERT HTML TABLES TO MARKDOWN"
      ],
      "metadata": {
        "id": "OlA4g7o9aqWD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# convert html tables into markdown\n",
        "\n",
        "for table in tables:\n",
        "    table.page_content = htmltabletomd.convert_table(table.metadata['text_as_html'])\n",
        "\n",
        "for table in tables:\n",
        "    print(table.page_content)\n",
        "    print()"
      ],
      "metadata": {
        "id": "2ItwFBemYnwP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Viwe extracted image\n",
        "\n",
        "! ls -l ./figures"
      ],
      "metadata": {
        "id": "kseMhj-daaB8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Image\n",
        "\n",
        "Image('./figures/figure-1-1.jpg')\n",
        "Image('./figures/figure-1-2.jpg')"
      ],
      "metadata": {
        "id": "e4XVYPigaaFG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Enter Open AI API Key\n",
        "\n",
        "\n",
        "from getpass import getpass\n",
        "\n",
        "OPENAI_KEY = getpass('Enter Open AI API Key: ')\n",
        "\n",
        "\n",
        "\n",
        "import os\n",
        "\n",
        "os.environ['OPENAI_API_KEY'] = OPENAI_KEY\n"
      ],
      "metadata": {
        "id": "yDPvkftHaaJs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Load Connection to LLM\n",
        "\n",
        "# Here we create a connection to ChatGPT to use later in our chains"
      ],
      "metadata": {
        "id": "gSv527eGaaM8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# Pass the API key explicitly to the ChatOpenAI constructor\n",
        "chatgpt = ChatOpenAI(model_name='gpt-4o', temperature=0, api_key=openai_key)"
      ],
      "metadata": {
        "id": "Qrg76SybabFc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### Text and Table summaries\n",
        "\n",
        "We will use GPT-4o to produce table and, text summaries.\n",
        "\n",
        "Text summaries are advised if using large chunk sizes (e.g., as set above, we use 4k token chunks).\n",
        "\n",
        "Summaries are used to retrieve raw tables and / or raw chunks of text."
      ],
      "metadata": {
        "id": "0_beExKgdI_5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.runnables import RunnablePassthrough"
      ],
      "metadata": {
        "id": "ZsgaNg47alHN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Prompt\n",
        "prompt_text = \"\"\"\n",
        "You are an assistant tasked with summarizing tables and text particularly for semantic retrieval.\n",
        "These summaries will be embedded and used to retrieve the raw text or table elements\n",
        "Give a detailed summary of the table or text below that is well optimized for retrieval.\n",
        "For any tables also add in a one line description of what the table is about besides the summary.\n",
        "Do not add redundant words like Summary.\n",
        "Just output the actual summary content.\n",
        "\n",
        "Table or text chunk:\n",
        "{element}\n",
        "\"\"\"\n",
        "prompt = ChatPromptTemplate.from_template(prompt_text)\n",
        "\n",
        "# Summary chain\n",
        "summarize_chain = (\n",
        "                    {\"element\": RunnablePassthrough()}\n",
        "                      |\n",
        "                    prompt\n",
        "                      |\n",
        "                    chatgpt\n",
        "                      |\n",
        "                    StrOutputParser() # extracts the response as text and returns it as a string\n",
        ")\n",
        "\n",
        "# Initialize empty summaries\n",
        "text_summaries = []\n",
        "table_summaries = []\n",
        "\n",
        "text_docs = [doc.page_content for doc in docs]\n",
        "table_docs = [table.page_content for table in tables]\n",
        "\n",
        "text_summaries = summarize_chain.batch(text_docs, {\"max_concurrency\": 5})\n",
        "table_summaries = summarize_chain.batch(table_docs, {\"max_concurrency\": 5})\n",
        "\n",
        "len(text_summaries), len(table_summaries)"
      ],
      "metadata": {
        "id": "U08n8Jh5aloq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# IMAGE SUMMARIZATION VIA LLM"
      ],
      "metadata": {
        "id": "7ouQefRLe_Ab"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import base64\n",
        "import os\n",
        "\n",
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "\n",
        "def encode_image(image_path):\n",
        "    \"\"\"Getting the base64 string\"\"\"\n",
        "    with open(image_path, \"rb\") as image_file:\n",
        "        return base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
        "\n",
        "\n",
        "def image_summarize(img_base64, prompt):\n",
        "    \"\"\"Make image summary\"\"\"\n",
        "    chat = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
        "\n",
        "    msg = chat.invoke(\n",
        "        [\n",
        "            HumanMessage(\n",
        "                content=[\n",
        "                    {\"type\": \"text\", \"text\": prompt},\n",
        "                    {\n",
        "                        \"type\": \"image_url\",\n",
        "                        \"image_url\": {\"url\": f\"data:image/jpeg;base64,{img_base64}\"},\n",
        "                    },\n",
        "                ]\n",
        "            )\n",
        "        ]\n",
        "    )\n",
        "    return msg.content\n",
        "\n",
        "\n",
        "def generate_img_summaries(path):\n",
        "    \"\"\"\n",
        "    Generate summaries and base64 encoded strings for images\n",
        "    path: Path to list of .jpg files extracted by Unstructured\n",
        "    \"\"\"\n",
        "\n",
        "    # Store base64 encoded images\n",
        "    img_base64_list = []\n",
        "\n",
        "    # Store image summaries\n",
        "    image_summaries = []\n",
        "\n",
        "    # Prompt\n",
        "    prompt = \"\"\"You are an assistant tasked with summarizing images for retrieval.\n",
        "                Remember these images could potentially contain graphs, charts or tables also.\n",
        "                These summaries will be embedded and used to retrieve the raw image for question answering.\n",
        "                Give a detailed summary of the image that is well optimized for retrieval.\n",
        "                Do not add additional words like Summary, This image represents, etc.\n",
        "             \"\"\"\n",
        "\n",
        "    # Apply to images\n",
        "    for img_file in sorted(os.listdir(path)):\n",
        "        if img_file.endswith(\".jpg\"):\n",
        "            img_path = os.path.join(path, img_file)\n",
        "            base64_image = encode_image(img_path)\n",
        "            img_base64_list.append(base64_image)\n",
        "            image_summaries.append(image_summarize(base64_image, prompt))\n",
        "\n",
        "    return img_base64_list, image_summaries\n",
        "\n",
        "\n",
        "# Image summaries\n",
        "IMG_PATH = './figures'\n",
        "imgs_base64, image_summaries = generate_img_summaries(IMG_PATH)"
      ],
      "metadata": {
        "id": "pjlZglSQeeqU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(imgs_base64), len(image_summaries)"
      ],
      "metadata": {
        "id": "SNhwZz6-efXl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(Image('./figures/figure-1-2.jpg'))"
      ],
      "metadata": {
        "id": "XknSJyEXf0uO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(Markdown(image_summaries[1]))"
      ],
      "metadata": {
        "id": "VEtNIqmqefkz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multi-vector retriever\n",
        "\n",
        "Use [multi-vector-retriever](https://python.langchain.com/docs/modules/data_connection/retrievers/multi_vector#summary) to index image (and / or text, table) summaries, but retrieve raw images (along with raw texts or tables)."
      ],
      "metadata": {
        "id": "XMWseLHAgWHe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Download and Install Redis as a DocStore\n",
        "\n",
        "You can use any other database or cache as a docstore to store the raw text, table and image elements"
      ],
      "metadata": {
        "id": "u3wJtJrrgbg6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%sh\n",
        "curl -fsSL https://packages.redis.io/gpg | sudo gpg --dearmor -o /usr/share/keyrings/redis-archive-keyring.gpg\n",
        "echo \"deb [signed-by=/usr/share/keyrings/redis-archive-keyring.gpg] https://packages.redis.io/deb $(lsb_release -cs) main\" | sudo tee /etc/apt/sources.list.d/redis.list\n",
        "sudo apt-get update  > /dev/null 2>&1\n",
        "sudo apt-get install redis-stack-server  > /dev/null 2>&1\n",
        "redis-stack-server --daemonize yes"
      ],
      "metadata": {
        "id": "AhCNxciAgQfF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Open AI Embedding Models\n",
        "\n",
        "LangChain enables us to access Open AI embedding models which include the newest models: a smaller and highly efficient `text-embedding-3-small` model, and a larger and more powerful `text-embedding-3-large` model."
      ],
      "metadata": {
        "id": "WuUyx-ZfglK_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "# details here: https://openai.com/blog/new-embedding-models-and-api-updates\n",
        "openai_embed_model = OpenAIEmbeddings(model='text-embedding-3-small')\n"
      ],
      "metadata": {
        "id": "bCLGT46Uefuz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Add to vectorstore & docstore\n",
        "\n",
        "Add raw docs and doc summaries to [Multi Vector Retriever](https://python.langchain.com/docs/modules/data_connection/retrievers/multi_vector#summary):\n",
        "\n",
        "* Store the raw texts, tables, and images in the `docstore` (here we are using Redis).\n",
        "* Store the texts, table summaries, and image summaries and their corresponding embeddings in the `vectorstore` (here we are using Chroma) for efficient semantic retrieval.\n",
        "* Connect them using a common `document_id`"
      ],
      "metadata": {
        "id": "vcKw0-9ng2YC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import uuid\n",
        "\n",
        "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
        "from langchain_community.storage import RedisStore\n",
        "from langchain_community.utilities.redis import get_client\n",
        "from langchain_chroma import Chroma\n",
        "from langchain_core.documents import Document\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "\n",
        "def create_multi_vector_retriever(\n",
        "    docstore, vectorstore, text_summaries, texts, table_summaries, tables, image_summaries, images\n",
        "):\n",
        "    \"\"\"\n",
        "    Create retriever that indexes summaries, but returns raw images or texts\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    id_key = \"doc_id\"\n",
        "\n",
        "    # Create the multi-vector retriever\n",
        "    retriever = MultiVectorRetriever(\n",
        "        vectorstore=vectorstore,\n",
        "        docstore=docstore,\n",
        "        id_key=id_key,\n",
        "    )\n",
        "\n",
        "    # Helper function to add documents to the vectorstore and docstore\n",
        "    def add_documents(retriever, doc_summaries, doc_contents):\n",
        "        doc_ids = [str(uuid.uuid4()) for _ in doc_contents]\n",
        "        summary_docs = [\n",
        "            Document(page_content=s, metadata={id_key: doc_ids[i]})\n",
        "            for i, s in enumerate(doc_summaries)\n",
        "        ]\n",
        "        retriever.vectorstore.add_documents(summary_docs)\n",
        "        retriever.docstore.mset(list(zip(doc_ids, doc_contents)))\n",
        "\n",
        "    # Add texts, tables, and images\n",
        "    # Check that text_summaries is not empty before adding\n",
        "    if text_summaries:\n",
        "        add_documents(retriever, text_summaries, texts)\n",
        "    # Check that table_summaries is not empty before adding\n",
        "    if table_summaries:\n",
        "        add_documents(retriever, table_summaries, tables)\n",
        "    # Check that image_summaries is not empty before adding\n",
        "    if image_summaries:\n",
        "        add_documents(retriever, image_summaries, images)\n",
        "\n",
        "    return retriever\n",
        "\n",
        "\n",
        "# The vectorstore to use to index the summaries and their embeddings\n",
        "chroma_db = Chroma(\n",
        "    collection_name=\"mm_rag\",\n",
        "    embedding_function=openai_embed_model,\n",
        "    collection_metadata={\"hnsw:space\": \"cosine\"},\n",
        ")\n",
        "\n",
        "# Initialize the storage layer - to store raw images, text and tables\n",
        "client = get_client('redis://localhost:6379')\n",
        "redis_store = RedisStore(client=client) # you can use filestore, memorystory, any other DB store also\n",
        "\n",
        "# Create retriever\n",
        "retriever_multi_vector = create_multi_vector_retriever(\n",
        "    redis_store,\n",
        "    chroma_db,\n",
        "    text_summaries,\n",
        "    text_docs,\n",
        "    table_summaries,\n",
        "    table_docs,\n",
        "    image_summaries,\n",
        "    imgs_base64,\n",
        ")"
      ],
      "metadata": {
        "id": "MJdWcOcNef3w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retriever_multi_vector"
      ],
      "metadata": {
        "id": "A-WR8JP6egAI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test Multimodal RAG Retriever\n"
      ],
      "metadata": {
        "id": "b3wcBTRriewU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import HTML, display, Image\n",
        "from PIL import Image\n",
        "import base64\n",
        "from io import BytesIO\n",
        "\n",
        "def plt_img_base64(img_base64):\n",
        "    \"\"\"Disply base64 encoded string as image\"\"\"\n",
        "    # Decode the base64 string\n",
        "    img_data = base64.b64decode(img_base64)\n",
        "    # Create a BytesIO object\n",
        "    img_buffer = BytesIO(img_data)\n",
        "    # Open the image using PIL\n",
        "    img = Image.open(img_buffer)\n",
        "    display(img)"
      ],
      "metadata": {
        "id": "rDO609vJegJF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CHECK RETRIEVAL\n",
        "query = \"Analyze the wildfires trend with acres burned over the years\"\n",
        "docs = retriever_multi_vector.invoke(query, limit=5)\n",
        "\n",
        "# We get 3 docs\n",
        "len(docs)\n",
        "\n",
        "docs"
      ],
      "metadata": {
        "id": "fdRTCPlTegSP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check retrieval\n",
        "query = \"Tell me about the percentage of residences burned by wildfires in 2022\"\n",
        "docs = retriever_multi_vector.invoke(query, limit=5)\n",
        "\n",
        "# We get 4 docs\n",
        "len(docs)\n",
        "docs"
      ],
      "metadata": {
        "collapsed": true,
        "id": "cMupvvvNegar"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Utilities to separate retrieved elements\n",
        "\n",
        "We need to bin the retrieved doc(s) into the correct parts of the GPT-4o prompt template.\n",
        "\n",
        "Here we need to have text, table elements as one set of inputs and image elements as the other set of inputs as both require separate prompts in GPT-4o."
      ],
      "metadata": {
        "id": "8aCIa_jNjcqJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import base64\n",
        "\n",
        "def looks_like_base64(sb):\n",
        "    \"\"\"Check if the string looks like base64\"\"\"\n",
        "    return re.match(\"^[A-Za-z0-9+/]+[=]{0,2}$\", sb) is not None\n",
        "\n",
        "\n",
        "def is_image_data(b64data):\n",
        "    \"\"\"\n",
        "    Check if the base64 data is an image by looking at the start of the data\n",
        "    \"\"\"\n",
        "    image_signatures = {\n",
        "        b\"\\xff\\xd8\\xff\": \"jpg\",\n",
        "        b\"\\x89\\x50\\x4e\\x47\\x0d\\x0a\\x1a\\x0a\": \"png\",\n",
        "        b\"\\x47\\x49\\x46\\x38\": \"gif\",\n",
        "        b\"\\x52\\x49\\x46\\x46\": \"webp\",\n",
        "    }\n",
        "    try:\n",
        "        header = base64.b64decode(b64data)[:8]  # Decode and get the first 8 bytes\n",
        "        for sig, format in image_signatures.items():\n",
        "            if header.startswith(sig):\n",
        "                return True\n",
        "        return False\n",
        "    except Exception:\n",
        "        return False\n",
        "\n",
        "\n",
        "def split_image_text_types(docs):\n",
        "    \"\"\"\n",
        "    Split base64-encoded images and texts\n",
        "    \"\"\"\n",
        "    b64_images = []\n",
        "    texts = []\n",
        "    for doc in docs:\n",
        "        # Check if the document is of type Document and extract page_content if so\n",
        "        if isinstance(doc, Document):\n",
        "            doc = doc.page_content.decode('utf-8')\n",
        "        else:\n",
        "            doc = doc.decode('utf-8')\n",
        "        if looks_like_base64(doc) and is_image_data(doc):\n",
        "            b64_images.append(doc)\n",
        "        else:\n",
        "            texts.append(doc)\n",
        "    return {\"images\": b64_images, \"texts\": texts}"
      ],
      "metadata": {
        "id": "3Wgi6tmkegiY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check retrieval\n",
        "query = \"Tell me detailed statistics of the top 5 years with largest wildfire acres burned\"\n",
        "docs = retriever_multi_vector.invoke(query, limit=5)\n",
        "\n",
        "# We get 3 docs\n",
        "len(docs)\n",
        "docs"
      ],
      "metadata": {
        "collapsed": true,
        "id": "f0sqaue8egpT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "is_image_data(docs[2].decode('utf-8'))"
      ],
      "metadata": {
        "id": "CUYVKzIYkBY6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "r = split_image_text_types(docs)\n",
        "r"
      ],
      "metadata": {
        "id": "amrZSeyKkOux"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BUILD END TO END RAG PIPELINE for MULTIMODAL RAG"
      ],
      "metadata": {
        "id": "QquyrlCCkZDe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from operator import itemgetter\n",
        "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "def multimodal_prompt_function(data_dict):\n",
        "    \"\"\"\n",
        "    Create a multimodal prompt with both text and image context.\n",
        "\n",
        "    This function formats the provided context from `data_dict`, which contains\n",
        "    text, tables, and base64-encoded images. It joins the text (with table) portions\n",
        "    and prepares the image(s) in a base64-encoded format to be included in a message.\n",
        "\n",
        "    The formatted text and images (context) along with the user question are used to\n",
        "    construct a prompt for GPT-4o\n",
        "    \"\"\"\n",
        "    formatted_texts = \"\\n\".join(data_dict[\"context\"][\"texts\"])\n",
        "    messages = []\n",
        "\n",
        "    # Adding image(s) to the messages if present\n",
        "    if data_dict[\"context\"][\"images\"]:\n",
        "        for image in data_dict[\"context\"][\"images\"]:\n",
        "            image_message = {\n",
        "                \"type\": \"image_url\",\n",
        "                \"image_url\": {\"url\": f\"data:image/jpeg;base64,{image}\"},\n",
        "            }\n",
        "            messages.append(image_message)\n",
        "\n",
        "    # Adding the text and tables for analysis\n",
        "    text_message = {\n",
        "        \"type\": \"text\",\n",
        "        \"text\": (\n",
        "            f\"\"\"You are an analyst tasked with understanding detailed information and trends\n",
        "                from text documents, data tables, and charts and graphs in images.\n",
        "                You will be given context information below which will be a mix of text, tables,\n",
        "                and images usually of charts or graphs.\n",
        "                Use this information to provide answers related to the user question.\n",
        "                Analyze all the context information including tables, text and images to generate the answer.\n",
        "                Do not make up answers, If the question context is not present in the document just say dont know the answer and in that case please dont generate any sources.\n",
        "                use the provided context documents below\n",
        "                and answer the question to the best of your ability.\n",
        "\n",
        "                User question:\n",
        "                {data_dict['question']}\n",
        "\n",
        "                Context documents:\n",
        "                {formatted_texts}\n",
        "\n",
        "                Answer:\n",
        "            \"\"\"\n",
        "        ),\n",
        "    }\n",
        "    messages.append(text_message)\n",
        "    return [HumanMessage(content=messages)]\n",
        "\n",
        "\n",
        "# Create RAG chain\n",
        "multimodal_rag = (\n",
        "        {\n",
        "            \"context\": itemgetter('context'),\n",
        "            \"question\": itemgetter('input'),\n",
        "        }\n",
        "            |\n",
        "        RunnableLambda(multimodal_prompt_function)\n",
        "            |\n",
        "        chatgpt\n",
        "            |\n",
        "        StrOutputParser()\n",
        ")\n",
        "\n",
        "# Pass input query to retriever and get context document elements\n",
        "retrieve_docs = (itemgetter('input')\n",
        "                    |\n",
        "                retriever_multi_vector\n",
        "                    |\n",
        "                RunnableLambda(split_image_text_types))\n",
        "\n",
        "# Below, we chain `.assign` calls. This takes a dict and successively\n",
        "# adds keys-- \"context\" and \"answer\"-- where the value for each key\n",
        "# is determined by a Runnable (function or chain executing at runtime).\n",
        "# This helps in also having the retrieved context along with the answer generated by GPT-4o\n",
        "multimodal_rag_w_sources = (RunnablePassthrough.assign(context=retrieve_docs)\n",
        "                                               .assign(answer=multimodal_rag)\n",
        ")"
      ],
      "metadata": {
        "id": "psHBxI5SkPCS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run RAG chain\n",
        "query = \"Tell me detailed statistics of the top 5 years with largest wildfire acres burned\"\n",
        "response = multimodal_rag_w_sources.invoke({'input': query})\n",
        "response"
      ],
      "metadata": {
        "id": "HXj7p1LckPQH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def multimodal_rag_qa(query):\n",
        "    response = multimodal_rag_w_sources.invoke({'input': query})\n",
        "    print('=='*50)\n",
        "    print('Answer:')\n",
        "    display(Markdown(response['answer']))\n",
        "    print('--'*50)\n",
        "    print('Sources:')\n",
        "    text_sources = response['context']['texts']\n",
        "    img_sources = response['context']['images']\n",
        "    for text in text_sources:\n",
        "        display(Markdown(text))\n",
        "        print()\n",
        "    for img in img_sources:\n",
        "        plt_img_base64(img)\n",
        "        print()\n",
        "    print('=='*50)"
      ],
      "metadata": {
        "id": "30zQPH8ykPgu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"Tell me detailed statistics of the top 5 years with largest wildfire acres burned\"\n",
        "multimodal_rag_qa(query)"
      ],
      "metadata": {
        "id": "GUl6dUHskPwa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run RAG chain\n",
        "query = \"Tell me about the percentage of residences burned by wildfires in 2022\"\n",
        "multimodal_rag_qa(query)"
      ],
      "metadata": {
        "id": "bkgNX3cVlYq3",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run RAG chain\n",
        "query = \"Analyze the wildfires trend with acres burned over the years\"\n",
        "multimodal_rag_qa(query)"
      ],
      "metadata": {
        "id": "ohDbzegdll2o",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run RAG chain\n",
        "query = \"which teams are the part of ICC t20 worldcup 2026\"\n",
        "multimodal_rag_qa(query)"
      ],
      "metadata": {
        "id": "u3LDdHykl-bA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}